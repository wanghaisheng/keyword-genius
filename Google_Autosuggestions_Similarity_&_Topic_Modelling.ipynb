{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hchl_zv1_etV"
      },
      "source": [
        "Howdy Notebooker (or soon to be notebooker)!\n",
        "\n",
        "This script automates Steve's interrogative Google Autosuggest strategy. This is where you Google a keyword, delete it and begin typing in \"interrogative words,\" e.g. words that begin senstences.\n",
        "\n",
        "All you need is a seed keyword like \"best mortgage rates\" and this Colab notebook does the rest!\n",
        "\n",
        "Sign up for weekly notes [seonotebook.com](https://seonotebook.com)\n",
        "\n",
        "Contact me for SEO consulting [steve@seonotebook.com](mailto://steve@seonotebook.com)\n",
        "\n",
        "Gigantic thanks to Max Geraci for creating this script. Don't miss his awesome tool, [Entities Checker](entitieschecker.com) and [Stacking.Cloud](https://stacking.cloud).\n",
        "\n",
        "Here's a rundown of how you can utilize this Colab notebook:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4ZeiiKglcZj"
      },
      "source": [
        "<!-- Logo Cell -->\n",
        "<div align=\"left\">\n",
        "  <a href=\"https://seonotebook.com/\">\n",
        "    <img src=\"https://sp-ao.shortpixel.ai/client/to_auto,q_glossy,ret_img/https://seonotebook.com/wp-content/uploads/2020/07/seonotebook-logo-rgb.png\" alt=\"SEO Notebook Logo\" width=\"280\" height=\"52\">\n",
        "  </a>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4qJRSHg1qWy"
      },
      "source": [
        "## **SEO Keyword Collection and Analysis Script Guide**\n",
        "\n",
        "This script is designed to collect a broad set of keywords starting from a seed keyword. Keywords are scraped from Google Autosuggest using various techniques, including:\n",
        "\n",
        "- **Alphabet technique**: Appending letters of the alphabet (or numbers 0-9) at different positions of the keyword (beginning, end, between words) to generate variations.\n",
        "- **Asterisk technique**: Using an asterisk as a wildcard to find keyword variations.\n",
        "- **Modifiers**: Incorporating over 200 interrogative and other modifiers to refine and expand the search.\n",
        "\n",
        "During the code execution, you will be prompted to enter the seed keyword, a proxy to ensure safe scraping (use the format http://username:password@host:port) to avoid the risk of Google banning your IP, and you will need to upload the Excel file containing the list of modifiers.\n",
        "\n",
        "The scraping can be done in a single step or recursively to find even more long-tail keywords, gradually moving away from the relevance of the seed keyword.\n",
        "\n",
        "**Output of this phase includes**:\n",
        "- `google_autocomplete_suggestions_output.csv`: This file contains the extracted keywords, organized into columns based on the scraping technique used.\n",
        "- `scraped_seed_keywords_list.csv`: A deduplicated list that merges various lists from the scraping phase. This consolidated list serves as the input for subsequent analyses, including **Semantic Similarity against a Context** and **Topic Modeling**.\n",
        "\n",
        "These outputs provide a comprehensive keyword set, paving the way for deeper analysis focused on identifying semantically relevant keywords and understanding user intent through topic trends.\n",
        "\n",
        "### Trimming Down Keywords Through Semantic Similarity\n",
        "To make the final list more manageable, a trim down approach based on semantic similarity is applied. The starting point is the seed keyword and two expansion keywords (sub-topics or semantically related keywords that define the main topic but can be lexically distant). The script will use GPT-4 to generate a semantically rich text from these keywords. **The seed and expansion keywords are crucial** as they set the context for the entire analysis. The generated context text is then used to compare against the scraped keyword list from Autosuggest to identify relevant keywords, based on a semantic similarity threshold.\n",
        "\n",
        "### Key Components of the Script\n",
        "The script uses:\n",
        "- **KMeans** for clustering,\n",
        "- **FastEmbed** for efficient vectorization,\n",
        "- **All-MiniLM-L6-v2** Transformer model for multi-language support.\n",
        "\n",
        "### Output and Downloads\n",
        "The lists of clustered keywords and excluded keywords, including their similarity scores, will be accessible for download in Excel format within the newly created 'Output' folder (`clustered_keywords.xlsx` and `excluded_keywords.xlsx`).\n",
        "\n",
        "### Topic Modeling and Insights\n",
        "Additionally, the script performs topic modeling on the scraped keyword list using **LDA with HDBSCAN**. The results, available in both tabular Excel format and graphical formats like word clouds, provide immediate insights into the predominant topics reflected in user searches captured by Google Autosuggest. These topic aggregations serve as a valuable tool for guiding the creation of a content plan or a hub of semantically related articles to cover the topic of interest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Qx3TdSfx4R"
      },
      "source": [
        "##**This section is to scrape Google Autocomplete Suggestions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hScrsiKGfW1F"
      },
      "source": [
        "###**Installation**\n",
        "Install all the necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9olo_AfWm16Y"
      },
      "outputs": [],
      "source": [
        "!pip install -q selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y-GudBpnXI9",
        "outputId": "f0965cc9-f4a7-45e3-c753-8b76de0a4a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.189.91.83)] [Connecting\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'chromium-chromedriver' instead of 'chromium-driver'\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install chromium-driver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM8Wcmkqn9rb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "6118835f-85f1-4c31-abdc-ee4a9f67215c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1f759c1655bd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output, project_id)\u001b[0m\n\u001b[1;32m    279\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_check_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CredentialType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_auth_ephem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m       _message.blocking_request(\n\u001b[0m\u001b[1;32m    282\u001b[0m           \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m           \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'auth_user_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzSlmglTrIE3"
      },
      "outputs": [],
      "source": [
        "from google.auth import default\n",
        "import gspread\n",
        "credential, _ = default()\n",
        "gc = gspread.authorize(credential)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLnIVsL7s2ev"
      },
      "outputs": [],
      "source": [
        "gsheet_filename = gc.create(\"scrape_google_suggestions\")\n",
        "sheet = gsheet_filename.sheet1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPGLW3CV6ztV"
      },
      "outputs": [],
      "source": [
        "sheet.append_row(['keyword', 'keyword_modifier_before_after', 'keyword_interrogatives', 'keyword_with_alphabet', 'keyword_with_asterisk',  'modifiers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUwnCL6uTalp"
      },
      "outputs": [],
      "source": [
        "gspread.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U945lEyGgnNC"
      },
      "source": [
        "###**Import all the needed libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK0zfDgPu-r3"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "import urllib\n",
        "import requests\n",
        "from csv import writer\n",
        "from itertools import zip_longest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntfz8Kougx5V"
      },
      "source": [
        "###Define variables and proxies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKODmoFOwRDy"
      },
      "outputs": [],
      "source": [
        "result_list = []\n",
        "keyword_alphabet_asterisk = []\n",
        "selenium_modifiers  =[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwvGfB-Vg8Y8"
      },
      "source": [
        "###The output should be stored in a CSV file. Before storing the output, define column names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_ONZ8qbdAiQ"
      },
      "outputs": [],
      "source": [
        "columns = ['keyword', 'keyword_modifier_before_after', 'keyword_interrogatives', 'keyword_with_alphabet', 'keyword_with_asterisk',  'modifiers']\n",
        "with open('google_autocomplete_suggestions_output.csv','w') as f:\n",
        "    writer_object=writer(f)\n",
        "    writer_object.writerow(columns)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F0ljuZ6hPhs"
      },
      "source": [
        "###Below function is to get autocomplete suggestions for list of modified_keyword [modifiers+keyword and keyword+modifiers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0g1vmw22vrD"
      },
      "outputs": [],
      "source": [
        "def scrape_using_request_modifiers(modified_keyword, proxy_list, language=\"en\", country=\"us\"):\n",
        "    \"\"\"Sends a GET request to Google's search suggest function and returns a list of suggestions (Simple version)\"\"\"\n",
        "    time.sleep(1)\n",
        "    headers = {'User-agent':'Mozilla/5.0'}\n",
        "    for proxy in proxy_list:\n",
        "        proxies = {\"http\": proxy, \"https\": proxy}\n",
        "        try:\n",
        "            encoded_keyword = urllib.parse.quote_plus(modified_keyword)\n",
        "\n",
        "            base_url = f\"http://google.com/complete/search?hl={language}&q={encoded_keyword}&json=t&client=serp\"\n",
        "            #response = requests.get(base_url, headers=headers, proxies=proxies)\n",
        "            response = requests.get(base_url, headers=headers)\n",
        "            if response.ok and response.content.strip():\n",
        "                content = json.loads(response.content)\n",
        "                if content and isinstance(content, list) and len(content) > 1:\n",
        "                    return content[1]  # Return suggestions without recursion\n",
        "            return []\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"An error occurred while connecting to the proxy : {str(e)}\")\n",
        "        else:\n",
        "            break\n",
        "    else:\n",
        "      print(f\"All proxies failed for keyword '{modified_keyword}'.\")\n",
        "      return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJZ10k7I2vrI"
      },
      "outputs": [],
      "source": [
        "def scrape_autocomplete_modifiers_kw(keyword, modifiers_list, proxy_list):\n",
        "    keyword_modifier_before_after = []\n",
        "    for modifier in modifiers_list:\n",
        "        modified_kw1 = modifier+' '+keyword\n",
        "        modified_kw2 = keyword+' '+modifier\n",
        "        modified_kw1_suggestions = scrape_using_request_modifiers(modified_kw1,proxy_list )\n",
        "        result_list.append({\n",
        "        \"keyword\": modified_kw1,\n",
        "        \"autocomplete_suggestions\": modified_kw1_suggestions\n",
        "        })\n",
        "        print(\"modifier \",modifier, ' ', modified_kw1_suggestions)\n",
        "        modified_kw2_suggestions = scrape_using_request_modifiers(modified_kw2,proxy_list)\n",
        "        result_list.append({\n",
        "        \"keyword\": modified_kw1,\n",
        "        \"autocomplete_suggestions\": modified_kw2_suggestions\n",
        "        })\n",
        "        print(\"modifier \",modifier, ' ', modified_kw2_suggestions)\n",
        "        keyword_modifier_before_after.append(modified_kw1_suggestions)\n",
        "        keyword_modifier_before_after.append(modified_kw2_suggestions)\n",
        "    return keyword_modifier_before_after"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEEfrA2D-YRT"
      },
      "source": [
        "###Below function is to get autocomplete suggestions for list of modifiers [interrogatives+keyword]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OKefsZu2vrM"
      },
      "outputs": [],
      "source": [
        "def scrape_autocomplete_interrogatives_kw(keyword, interrogatives_list, proxy_list, language=\"en\"):\n",
        "    \"\"\"Sends a GET request to Google's search suggest function and returns a list of suggestions (Simple version)\"\"\"\n",
        "    keyword_interrogatives = []\n",
        "    for interrogatives in interrogatives_list:\n",
        "        modified_keyword = interrogatives + ' '+keyword\n",
        "        time.sleep(1)\n",
        "        headers = {'User-agent':'Mozilla/5.0'}\n",
        "        for proxy in proxy_list:\n",
        "            proxies = {\"http\": proxy, \"https\": proxy}\n",
        "            try:\n",
        "                encoded_keyword = urllib.parse.quote_plus(modified_keyword)\n",
        "\n",
        "                base_url = f\"http://google.com/complete/search?hl={language}&q={encoded_keyword}&json=t&client=serp\"\n",
        "                #response = requests.get(base_url, headers=headers, proxies=proxies)\n",
        "                response = requests.get(base_url, headers=headers)\n",
        "                if response.ok and response.content.strip():\n",
        "                    content = json.loads(response.content)\n",
        "                    if content and isinstance(content, list) and len(content) > 1:\n",
        "                        print(\"interrogatives \",interrogatives+' '+str(content[1]))\n",
        "                        result_list.append({\n",
        "                        \"keyword\": modified_keyword,\n",
        "                        \"autocomplete_suggestions\": content[1]\n",
        "                        })\n",
        "                    keyword_interrogatives.append(content[1])\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"An error occurred while connecting to the proxy : {str(e)}\")\n",
        "            else:\n",
        "              break\n",
        "        else:\n",
        "          print(f\"All proxies failed for keyword '{keyword}'.\")\n",
        "          return []\n",
        "    return keyword_interrogatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJEv-fecikMd"
      },
      "source": [
        "###Below function is to get autocomplete suggestions for list of modifiers [alphabet+keyword and astersisk+keyword]. Alphabets and asterisk can be placed in start,middle or end of the keyword depending on User's choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiMLwKiL2vrQ"
      },
      "outputs": [],
      "source": [
        "def make_request_alphabet_asterisk(modified_keyword, depth, proxy_list, max_depth, language=\"en\"):\n",
        "    time.sleep(1)\n",
        "    headers = {'User-agent':'Mozilla/5.0'}\n",
        "    for proxy in proxy_list:\n",
        "        proxies = {\"http\": proxy, \"https\": proxy}\n",
        "        try:\n",
        "            encoded_keyword = urllib.parse.quote_plus(modified_keyword)\n",
        "\n",
        "            base_url = f\"http://google.com/complete/search?hl={language}&q={encoded_keyword}&json=t&client=serp\"\n",
        "            #response = requests.get(base_url, headers=headers, proxies=proxies)\n",
        "            response = requests.get(base_url, headers=headers)\n",
        "            if response.ok and response.content.strip():\n",
        "                content = json.loads(response.content)\n",
        "                if content and isinstance(content, list) and len(content) > 1:\n",
        "                    keyword_alphabet_asterisk.append(content[1])\n",
        "                    result_list.append({\n",
        "                        \"keyword\": modified_keyword,\n",
        "                        \"autocomplete_suggestions\": content[1]\n",
        "                    })\n",
        "                    print(\"alphabet/digits/asterisk \",modified_keyword+' '+str(content[1]))\n",
        "                    if depth < max_depth:\n",
        "                            for suggestion in content[1]:\n",
        "                                make_request_alphabet_asterisk(suggestion, depth+1, max_depth, language=1)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"An error occurred while connecting to the proxy : {str(e)}\")\n",
        "        else:\n",
        "          break\n",
        "    else:\n",
        "      print(f\"All proxies failed for keyword '{modified_keyword}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7etcdwL42vrT"
      },
      "outputs": [],
      "source": [
        "def scrape_autocomplete_alphabet(keyword, max_depth, insert_option, proxy_list, language=\"en\"):\n",
        "    \"\"\"Sends a GET request to Google's search suggest function and returns a list of suggestions (Simple version)\"\"\"\n",
        "    \"\"\"only when insert_option is end we have recursive approach, otherwise it is simple\"\"\"\n",
        "    for char in 'abcdefghijklmnopqrstuvwxyz123456789':\n",
        "        depth = 0\n",
        "        if insert_option in ['end', 'all']:\n",
        "            modified_keyword = keyword +' '+char\n",
        "            make_request_alphabet_asterisk(modified_keyword, depth,proxy_list, max_depth,language=\"en\")\n",
        "        if insert_option in ['start', 'all']:\n",
        "            modified_keyword = char + ' ' + keyword\n",
        "            make_request_alphabet_asterisk(modified_keyword, depth,proxy_list, max_depth=0, language=\"en\")\n",
        "        if insert_option in ['middle', 'all'] and ' ' in keyword:\n",
        "            words = keyword.split()\n",
        "            for i in range(1, len(words)):\n",
        "                modified_keyword = ' '.join(words[:i]) + ' ' + char + ' ' + ' '.join(words[i:])\n",
        "                make_request_alphabet_asterisk(modified_keyword, depth,proxy_list, max_depth=0, language=\"en\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def scrape_autocomplete_asterisk(keyword, max_depth, insert_option,proxy_list, language=\"en\"):\n",
        "    \"\"\"Sends a GET request to Google's search suggest function and returns a list of suggestions (Simple version)\"\"\"\n",
        "    \"\"\"only when insert_option is end we have recursive approach, otherwise it is simple\"\"\"\n",
        "    depth=0\n",
        "    if insert_option in ['end', 'all']:\n",
        "        modified_keyword = keyword +' '+'*'\n",
        "        make_request_alphabet_asterisk(modified_keyword, depth, proxy_list,max_depth, language=\"en\")\n",
        "    if insert_option in ['start', 'all']:\n",
        "        modified_keyword = '*' + ' ' + keyword\n",
        "        make_request_alphabet_asterisk(modified_keyword, depth, proxy_list,max_depth=0, language=\"en\")\n",
        "    if insert_option in ['middle', 'all'] and ' ' in keyword:\n",
        "        words = keyword.split()\n",
        "        for i in range(1, len(words)):\n",
        "            modified_keyword = ' '.join(words[:i]) + ' ' + '*' + ' ' + ' '.join(words[i:])\n",
        "            make_request_alphabet_asterisk(modified_keyword, depth,proxy_list, max_depth=0, language=\"en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlcPsqc6i65n"
      },
      "source": [
        "###Below function is to get autocomplete suggestions for list of modifiers [seed_keyword, modifiers alone] using Selenium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1OQcX0F2vrW"
      },
      "outputs": [],
      "source": [
        "def scrape_modifiers_suggestions_selenium(keyword, modifiers_list, index):\n",
        "\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--verbose\")\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--disable-gpu')\n",
        "    options.add_argument(\"--window-size=1920, 1200\")\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    driver.get(\"https://www.google.com\")\n",
        "\n",
        "\n",
        "    search = driver.find_element(by=By.NAME, value=\"q\")\n",
        "    search.send_keys(keyword)\n",
        "    search.send_keys(Keys.RETURN)\n",
        "\n",
        "    suggestions = driver.find_element(by=By.TAG_NAME, value=\"textarea\")\n",
        "    suggestions = suggestions.click()\n",
        "\n",
        "\n",
        "    driver.implicitly_wait(10)\n",
        "    html_list = driver.find_element(by=By.ID, value=\"searchform\")\n",
        "    items = html_list.find_elements(by=By.TAG_NAME, value=\"li\")\n",
        "\n",
        "    if index == 0:\n",
        "        keyword_autocomplete = []\n",
        "        for item in items:\n",
        "            if item.text!='':\n",
        "                keyword_autocomplete.append(item.text)\n",
        "\n",
        "        print(keyword_autocomplete)\n",
        "        result_list.append({\n",
        "            \"keyword\": keyword,\n",
        "            \"autocomplete_suggestions\": keyword_autocomplete\n",
        "        })\n",
        "        selenium_modifiers.append(keyword_autocomplete)\n",
        "\n",
        "    driver.implicitly_wait(2)\n",
        "\n",
        "    ############################# *****modifier Technique***** #########################\n",
        "    for modifier in modifiers_list:\n",
        "        driver.find_element(by=By.TAG_NAME, value=\"textarea\").clear()\n",
        "\n",
        "        add_keyword = driver.find_element(by=By.NAME, value=\"q\")\n",
        "        add_keyword.send_keys(modifier)\n",
        "\n",
        "\n",
        "        time.sleep(5)\n",
        "        items = driver.find_element(By.XPATH, \"//ul[@role='listbox']\").find_elements(By.XPATH, \"//li[@role='presentation']\")\n",
        "\n",
        "        keyword_autocomplete = []\n",
        "        for item in items:\n",
        "            if item.text!='':\n",
        "                keyword_autocomplete.append(item.text)\n",
        "        print(\"modifier list \",modifier, ' ',keyword_autocomplete)\n",
        "        result_list.append({\n",
        "            \"keyword\": modifier,\n",
        "            \"autocomplete_suggestions\": keyword_autocomplete\n",
        "        })\n",
        "        selenium_modifiers.append(keyword_autocomplete)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDmHTnb6k6mw"
      },
      "source": [
        "###Invoke all the functions mentioned above. The list of modifiers should be sent in batches to Selenium requests because Google imposes a rate limit when it exceeds 20-25 requests in Selenium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWKYUTrP2vrZ"
      },
      "outputs": [],
      "source": [
        "def scrape_autocomplete(keyword, modifiers_list, interrogatives_list, max_depth,insert_option, batch_size, proxy_list):\n",
        "\n",
        "    keyword_modifiers_before_after = scrape_autocomplete_modifiers_kw(keyword, modifiers_list,proxy_list)\n",
        "    keyword_interrogatives = scrape_autocomplete_interrogatives_kw(keyword, interrogatives_list,proxy_list)\n",
        "    scrape_autocomplete_alphabet(keyword, max_depth, insert_option,proxy_list, language=\"en\")\n",
        "    keyword_alphabet = keyword_alphabet_asterisk.copy()\n",
        "    keyword_alphabet_asterisk.clear()\n",
        "    scrape_autocomplete_asterisk(keyword, max_depth, insert_option,proxy_list, language=\"en\")\n",
        "    keyword_asterisk = keyword_alphabet_asterisk.copy()\n",
        "    keyword_alphabet_asterisk.clear()\n",
        "\n",
        "    batch_num = len(modifiers_list) // batch_size\n",
        "    for batch in range(batch_num+1):\n",
        "        start_index = batch * batch_num\n",
        "        end_index = start_index+batch_num\n",
        "        sub_modifiers = modifiers_list[start_index:end_index]\n",
        "        #print(\"start index \",start_index, 'end index', end_index, sub_modifiers)\n",
        "        scrape_modifiers_suggestions_selenium(keyword, sub_modifiers, batch)\n",
        "\n",
        "        #below section is to get remaining sub modifiers that were not part of batches\n",
        "        if batch==batch_num:\n",
        "            sub_modifiers = modifiers_list[end_index:]\n",
        "            #print(sub_modifiers)\n",
        "            scrape_modifiers_suggestions_selenium(keyword, sub_modifiers, batch)\n",
        "\n",
        "    #to convert 2D list to 1D list and remove any duplicates if any\n",
        "    kw_modifier_before_after_1d = list(set([keyword for sublist in keyword_modifiers_before_after for keyword in sublist]))\n",
        "    kw_interrogatives_1d = list(set([keyword for sublist in keyword_interrogatives for keyword in sublist]))\n",
        "    kw_alphabet_1d = list(set([keyword for sublist in keyword_alphabet for keyword in sublist]))\n",
        "    kw_asterisk_1d = list(set([keyword for sublist in keyword_asterisk for keyword in sublist]))\n",
        "    kw_selenium_modifiers_1d = list(set([keyword for sublist in selenium_modifiers for keyword in sublist]))\n",
        "\n",
        "    all_combined = [[keyword], kw_modifier_before_after_1d, kw_interrogatives_1d, kw_alphabet_1d, kw_asterisk_1d, kw_selenium_modifiers_1d]\n",
        "    export_data = zip_longest(*all_combined, fillvalue = '')\n",
        "    #output= [keyword, kw_modifier_before_after_1d, kw_interrogatives_1d, kw_alphabet_1d, kw_asterisk_1d, kw_selenium_modifiers_1d]\n",
        "    with open('google_autocomplete_suggestions_output.csv','a') as f:\n",
        "        writer_obj = writer(f)\n",
        "        writer_obj.writerows(export_data)\n",
        "        f.close()\n",
        "    #sheet.append_row([keyword, str(keyword_modifiers_before_after), str(keyword_interrogatives), str(keyword_alphabet), str(keyword_asterisk), str(selenium_modifiers)])\n",
        "    selenium_modifiers.clear()\n",
        "    return result_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4M0Cxhj2vrb"
      },
      "outputs": [],
      "source": [
        "def read_modifiers_list(filename):\n",
        "    modifier_data = pd.read_excel(filename, sheet_name=\"modifiers\")\n",
        "    interrogatives = pd.read_excel(filename, sheet_name=\"interrogative words\")\n",
        "    return modifier_data[\"modifiers\"].unique().tolist(), interrogatives[\"interrogatives\"].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UUg2qRul-6D"
      },
      "source": [
        "##Call the scrape_autocomplete() for all the keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ktusizfm2vrc"
      },
      "outputs": [],
      "source": [
        "def get_autocomplete_results(seed_keywords,insert_option,filename,max_depth,proxy_list,batch_size):\n",
        "    modifier_list, interrogatives_list = read_modifiers_list(filename)\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        for keyword in seed_keywords:\n",
        "            result_list = executor.submit(scrape_autocomplete,keyword, modifier_list, interrogatives_list, max_depth,insert_option, batch_size,proxy_list).result()\n",
        "            with open(keyword+\".json\", 'w') as f:\n",
        "                json.dump(result_list, f)\n",
        "            result_list.clear()\n",
        "        executor.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxHycjg1mGT7"
      },
      "source": [
        "###Files from Drive should be mounted to the current directory in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBP83aK05CLo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVTn_rfz2vre"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    keyword = str(input(\"Please enter the keyword: \"))\n",
        "    print(\"Enter the proxy in the format: http://username:password@host:port \")\n",
        "    print(\"The proxy can either be a single proxy or multiple. If multiple, please enter every proxies with space [for eg: proxy1 proxy2]\")\n",
        "    proxy = str(input())\n",
        "    proxy_list = list(map(str, proxy.split()))\n",
        "    print(\"Please choose an approach:\")\n",
        "    print(\"1. Simple\")\n",
        "    print(\"2. Recursive\")\n",
        "    approach_choice = int(input(\"Enter your choice: \")) - 1\n",
        "\n",
        "    # New code block to get user's choice on character insertion position\n",
        "    if approach_choice == 0:  # if 'Simple' approach is chosen\n",
        "        print(\"Please choose where to insert the characters:\")\n",
        "        print(\"1. Start\")\n",
        "        print(\"2. End\")\n",
        "        print(\"3. Middle\")\n",
        "        print(\"4. All\")\n",
        "        insert_option_choice = int(input(\"Enter your choice: \")) - 1\n",
        "        insert_options = ['start', 'end', 'middle', 'all']\n",
        "        chosen_insert_option = insert_options[insert_option_choice]\n",
        "    else:\n",
        "        chosen_insert_option = 'end'  # default to 'end' for 'Recursive' approach\n",
        "\n",
        "    if approach_choice==0:\n",
        "        max_depth = 0\n",
        "    else:\n",
        "        max_depth = int(input(\"Enter max depth for recursive approach: Please enter any from 1-5\"))\n",
        "        #max_depth=1\n",
        "\n",
        "    print('Upload query modifiers list')\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "      print('This file is mandatory!')\n",
        "      return\n",
        "\n",
        "    filename = next(iter(uploaded))\n",
        "\n",
        "    get_autocomplete_results([keyword], chosen_insert_option, filename,max_depth, proxy_list, batch_size=10)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzJOdmTlUlcB"
      },
      "source": [
        "##The code below is designed to further process the output file for use as input in semantic similarity analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLy1CQKZTlnY"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "import ast\n",
        "\n",
        "df = pd.read_csv(\"google_autocomplete_suggestions_output.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN9OiRiFdzMo"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKBXVCYMVANX"
      },
      "outputs": [],
      "source": [
        "global_list = []\n",
        "seed_keyword = df[\"keyword\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieqtVUTDVBcV"
      },
      "outputs": [],
      "source": [
        "# df = df.drop([\"keyword\"], axis=1)\n",
        "for col in df.columns:\n",
        "  rm_null = df[col].dropna()\n",
        "  col_values = [elem for elem in rm_null]\n",
        "  global_list.append(col_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e1ZH35igE12"
      },
      "outputs": [],
      "source": [
        "global_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SJO6JlSVzYc"
      },
      "outputs": [],
      "source": [
        "#flatten 2D list to 1D list\n",
        "scraped_keywords = [keyword for sublist in global_list for keyword in sublist]\n",
        "scraped_keywords = list(set(scraped_keywords))\n",
        "print(\"Scraped keywords \\n\", scraped_keywords)\n",
        "output = pd.DataFrame({\"scraped keywords\": scraped_keywords})\n",
        "output.to_csv(\"scraped_seed_keywords_list.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn5yGykkg41-"
      },
      "outputs": [],
      "source": [
        "len(scraped_keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EFMqeWQiUWK"
      },
      "source": [
        "# **Semantic Similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KP-jmjvcDrS"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai fastembed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEg_WPxKi1WE"
      },
      "source": [
        "The code below creates the directory 'output'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-_uSKKligi3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Specify the directory path\n",
        "directory = \"output\"\n",
        "# Check if the directory already exists\n",
        "if not os.path.exists(directory):\n",
        "    # Create the directory\n",
        "    os.makedirs(directory)\n",
        "    print(\"Directory created successfully!\")\n",
        "else:\n",
        "    print(\"Directory already exists!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFqkManujMoy"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from fastembed import TextEmbedding\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import silhouette_score\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "import ipywidgets as wg\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lltQ3PDJjPBA"
      },
      "outputs": [],
      "source": [
        "# Load keywords from an Excel file\n",
        "def load_keywords(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    keywords_list = []\n",
        "    for column in df.columns:\n",
        "        keywords_list.extend(df[column].dropna().tolist())\n",
        "    return keywords_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23EvhaHFjRok"
      },
      "outputs": [],
      "source": [
        "def generate_context_gpt(keyword, expand_kws):\n",
        "    prompt = \"Generate an exploratory text that densely incorporates terminology and concepts related to {seed_KW}\"\n",
        "    if len(expand_kws) > 0:\n",
        "        prompt += \", interweaving related topics such as {expand_KW1}\"\n",
        "    if len(expand_kws) > 1:\n",
        "        prompt += \" and {expand_KW2}\"\n",
        "    prompt += \". Aim to cover a broad spectrum of entities, terms, and sub-topics semantically connected to the primary topic, providing a rich set of data points for semantic analysis. The narrative is not crucial here. The goal is to include the 'Central Entities' and as many related terms, keywords, entities, and sub-topics as possible to create a rich dictionary. Generate the requested text without any unrelated comments.\"\n",
        "\n",
        "    format_dict = {'seed_KW': keyword}\n",
        "    if len(expand_kws) > 0:\n",
        "        format_dict['expand_KW1'] = expand_kws[0]\n",
        "    if len(expand_kws) > 1:\n",
        "        format_dict['expand_KW2'] = expand_kws[1]\n",
        "\n",
        "    prompt = prompt.format(**format_dict)\n",
        "\n",
        "    try:\n",
        "        client = OpenAI()\n",
        "        result = client.chat.completions.create(\n",
        "            model=\"gpt-4-1106-preview\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\",\n",
        "                 \"content\": \"You are an assistant and you should generate text for the provided keyword. You always follow the instructions.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=abs(4000 - len(prompt)),\n",
        "            seed=123\n",
        "        )\n",
        "        generated_prompt = result.choices[0].message.content\n",
        "        return generated_prompt\n",
        "    except openai.RateLimitError as e:\n",
        "        sleep_duration = int(e.headers.get('Retry-After', 30))\n",
        "        print(f\"Rate limit exceeded. Sleeping for {sleep_duration} seconds before retrying...\")\n",
        "        time.sleep(sleep_duration)\n",
        "        return None\n",
        "    except openai.APIError as e:\n",
        "        print(f\"Failed to generate prompt due to error: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRV0bxKFjTz9"
      },
      "outputs": [],
      "source": [
        "def save_context_to_file(context, filename):\n",
        "  # Specify the directory path\n",
        "  directory = \"contents\"\n",
        "  # Check if the directory already exists\n",
        "  if not os.path.exists(directory):\n",
        "      # Create the directory\n",
        "      os.makedirs(directory)\n",
        "      with open(\"./contents/\" + filename, 'w') as f:\n",
        "          f.write(context)\n",
        "      print(\"Directory created successfully!\")\n",
        "  else:\n",
        "      print(\"Directory already exists!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgRq7YFYjWqC"
      },
      "outputs": [],
      "source": [
        "def load_context_gpt(seed_keyword, expand_kws):\n",
        "    context = generate_context_gpt(seed_keyword, expand_kws)\n",
        "    if context:\n",
        "        context_filename = f\"{seed_keyword}_context.txt\"\n",
        "        save_context_to_file(context, context_filename)\n",
        "        return context\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFPrKUrYjaSe"
      },
      "outputs": [],
      "source": [
        "# Generate embeddings for a list of texts using FastEmbed\n",
        "def generate_embeddings(texts):\n",
        "    model = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    return list(model.embed(texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ_3C8nCjc2b"
      },
      "outputs": [],
      "source": [
        "# Cluster embeddings using K-Means\n",
        "def cluster_embeddings(embeddings, num_clusters=5):\n",
        "    kmeans = KMeans(n_clusters=num_clusters)  # Adjust the number of clusters as appropriate\n",
        "    return kmeans.fit_predict(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6sg25gVjfBM"
      },
      "outputs": [],
      "source": [
        "def contextual_relevance_analysis(keyword_embeddings, context_embedding, clusters):\n",
        "    # Calculate cosine similarity between keyword embeddings and the context embedding\n",
        "    # Note: context_embedding should be reshaped to fit the expected dimensionality if it's a single vector\n",
        "    similarities = cosine_similarity(keyword_embeddings, context_embedding.reshape(1, -1))\n",
        "    relevance_scores = []\n",
        "\n",
        "    # Iterate over unique cluster IDs to calculate mean relevance score per cluster\n",
        "    for cluster_id in set(clusters):\n",
        "        cluster_indices = [i for i, cluster in enumerate(clusters) if cluster == cluster_id]\n",
        "        # Extract similarities for keywords within the current cluster\n",
        "        cluster_similarities = similarities[cluster_indices, 0]  # Assuming similarities is a 2D array\n",
        "        # Calculate mean similarity score for the current cluster\n",
        "        cluster_relevance_score = np.mean(cluster_similarities)\n",
        "        # Store the mean relevance score along with the cluster ID\n",
        "        relevance_scores.append((cluster_id, cluster_relevance_score))\n",
        "\n",
        "    # Sort the relevance scores list by the relevance score in descending order\n",
        "    relevance_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return relevance_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AelDP9i7jhMl"
      },
      "outputs": [],
      "source": [
        "def optimal_cluster_number(embeddings):\n",
        "    best_score = -1\n",
        "    best_n_clusters = 2\n",
        "    for n_clusters in range(2, min(len(embeddings), 10) + 1):\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "        score = silhouette_score(embeddings, labels)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_n_clusters = n_clusters\n",
        "    return best_n_clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xJyMAjvjnuA"
      },
      "outputs": [],
      "source": [
        "# Main script logic\n",
        "def main():\n",
        "  openai_api_key = str(input(\"Please enter Open AI API Key: \"))\n",
        "  os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "  keywords_list = load_keywords('scraped_seed_keywords_list.csv')\n",
        "  print(f\"Loaded {len(keywords_list)} keywords.\")\n",
        "\n",
        "  seed_keyword = str(input(\"Please enter seed keyword: \"))\n",
        "  print(\"Please enter expand keywords related to the seed keyword. Enter it in the format below.\")\n",
        "  print(\"For example: expand_kw1,expand_kw2 \")\n",
        "  expand_kws = str(input())\n",
        "  expand_kws = list(map(str, expand_kws.split(',')))\n",
        "  print(\"Expand keywords are \", expand_kws)\n",
        "\n",
        "  similarity_threshold = float(input(\"Please enter threshold (range from min: 0.1 to max: 0.8) for semantic similarity: \"))\n",
        "  print(\"Similarity threshold: \",similarity_threshold)\n",
        "\n",
        "  context_text = load_context_gpt(seed_keyword, expand_kws)\n",
        "  if not context_text:\n",
        "      print(f\"No context text generated for {seed_keyword}. Skipping.\")\n",
        "      #continue\n",
        "\n",
        "  context_embedding = generate_embeddings([context_text])[0]\n",
        "  keyword_embeddings = generate_embeddings(keywords_list)\n",
        "\n",
        "  # Calculate similarity for all keywords\n",
        "  all_keywords_similarity = cosine_similarity(keyword_embeddings, context_embedding.reshape(1, -1)).flatten()\n",
        "\n",
        "  # Correctly determining the optimal number of clusters and using it\n",
        "  optimal_n_clusters = optimal_cluster_number(keyword_embeddings)\n",
        "  clusters = KMeans(n_clusters=optimal_n_clusters, random_state=42).fit_predict(keyword_embeddings)\n",
        "  print(f\"Optimal number of clusters determined to be {optimal_n_clusters}.\")\n",
        "\n",
        "  # Separate selected and excluded keywords based on the similarity threshold\n",
        "  results = []\n",
        "  excluded_keywords_with_scores = []\n",
        "\n",
        "  for i, keyword in enumerate(keywords_list):\n",
        "      similarity_score = all_keywords_similarity[i]\n",
        "      if similarity_score >= similarity_threshold:\n",
        "          results.append({\n",
        "              'Keyword': keyword,\n",
        "              'Cluster': clusters[i],\n",
        "              'Similarity to Context': similarity_score\n",
        "          })\n",
        "      else:\n",
        "          excluded_keywords_with_scores.append((keyword, similarity_score))\n",
        "\n",
        "  # Debugging: Verbose output of keywords selection\n",
        "  print(\"Debugging output for keyword selection process:\")\n",
        "\n",
        "\n",
        "\n",
        "  if results:\n",
        "      results_df = pd.DataFrame(results)\n",
        "      print(f\"Results for '{seed_keyword}':\", results_df)\n",
        "      results_df.to_excel(f'./output/clustered_keywords_{seed_keyword}.xlsx', index=False)\n",
        "  else:\n",
        "      print(f\"No results to save for '{seed_keyword}'.\")\n",
        "\n",
        "  if excluded_keywords_with_scores:\n",
        "      excluded_kw_df = pd.DataFrame(excluded_keywords_with_scores, columns=['Excluded Keywords', 'Similarity Score'])\n",
        "      # print(f\"Excluded Keywords for '{seed_keyword}':\", excluded_kw_df)\n",
        "      excluded_kw_df.to_excel(f\"./output/excluded_keywords_{seed_keyword}.xlsx\", index=False)\n",
        "  else:\n",
        "      print(f\"No excluded keywords to save for '{seed_keyword}'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGRcMPzskVC6"
      },
      "source": [
        "# **LDA Topic Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOfNvlrOkZCL"
      },
      "outputs": [],
      "source": [
        "!pip install -q hdbscan wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYP9wPD5lkE4"
      },
      "outputs": [],
      "source": [
        "directory = \"LDA_output_images\"\n",
        "# Check if the directory already exists\n",
        "if not os.path.exists(directory):\n",
        "    # Create the directory\n",
        "    os.makedirs(directory)\n",
        "    print(\"Directory created successfully!\")\n",
        "else:\n",
        "    print(\"Directory already exists!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyUkmmRcmB1u"
      },
      "outputs": [],
      "source": [
        "directory = \"LDA_output_files\"\n",
        "# Check if the directory already exists\n",
        "if not os.path.exists(directory):\n",
        "    # Create the directory\n",
        "    os.makedirs(directory)\n",
        "    print(\"Directory created successfully!\")\n",
        "else:\n",
        "    print(\"Directory already exists!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBDjwQCTksJv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import hdbscan\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwaeKDcektA3"
      },
      "outputs": [],
      "source": [
        "def preprocess_keyword(keyword):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    keyword = keyword.lower()\n",
        "    keyword = ' '.join([lemmatizer.lemmatize(word) for word in keyword.split() if word not in stop_words])\n",
        "    return keyword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7g4H1cDks8m"
      },
      "outputs": [],
      "source": [
        "def read_whole_list(filename):\n",
        "    print(f\"Reading whole list from {filename}\")\n",
        "    df = pd.read_csv(filename)\n",
        "    return [preprocess_keyword(kw) for kw in df['scraped keywords'].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbDgq_E3kyO_"
      },
      "outputs": [],
      "source": [
        "def cluster_keywords(whole_list, min_cluster_size, min_samples, cluster_selection_epsilon):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(whole_list)\n",
        "\n",
        "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples,\n",
        "                                cluster_selection_epsilon=cluster_selection_epsilon)\n",
        "    cluster_labels = clusterer.fit_predict(tfidf_matrix)\n",
        "\n",
        "    df_clusters = pd.DataFrame({'Keyword': whole_list, 'Cluster': cluster_labels})\n",
        "    return df_clusters, tfidf_matrix, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9lM9XdSkyDq"
      },
      "outputs": [],
      "source": [
        "def topic_modeling(tfidf_matrix, n_components, max_iter, learning_method):\n",
        "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=max_iter,\n",
        "                                    learning_method=learning_method, random_state=42)\n",
        "    lda.fit(tfidf_matrix)\n",
        "    return lda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qlja-byZkx5Q"
      },
      "outputs": [],
      "source": [
        "def visualize_clusters(df_clusters, tfidf_matrix):\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    tsne_embeddings = tsne.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    scatter = plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=df_clusters['Cluster'], cmap='viridis')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.title('Keyword Clusters')\n",
        "    plt.savefig('./LDA_output_images/keyword_clusters.png')\n",
        "    plt.close()\n",
        "\n",
        "    cluster_sizes = df_clusters['Cluster'].value_counts()\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(cluster_sizes.index, cluster_sizes.values)\n",
        "    plt.xlabel('Cluster')\n",
        "    plt.ylabel('Number of Keywords')\n",
        "    plt.title('Keyword Cluster Sizes')\n",
        "    plt.savefig('./LDA_output_images/cluster_sizes.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC6xrTqYk4cH"
      },
      "outputs": [],
      "source": [
        "def visualize_topics(lda, vectorizer, n_top_words=20):\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_keywords = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(top_keywords))\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Topic {topic_idx + 1}')\n",
        "        plt.savefig(f'./LDA_output_images/topic_{topic_idx + 1}_wordcloud.png')\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaON8yfak8dY"
      },
      "outputs": [],
      "source": [
        "def create_output_files(df_clusters, lda, vectorizer):\n",
        "    n_top_words = 10\n",
        "\n",
        "    # Assign topics to keywords\n",
        "    topic_probs = lda.transform(vectorizer.transform(df_clusters['Keyword']))\n",
        "    df_clusters['Topic'] = topic_probs.argmax(axis=1) + 1\n",
        "\n",
        "    # Keyword Clusters worksheet\n",
        "    df_keyword_clusters = df_clusters[['Keyword', 'Cluster', 'Topic']]\n",
        "    df_keyword_clusters.to_excel('./LDA_output_files/output.xlsx', sheet_name='Keyword Clusters', index=False)\n",
        "\n",
        "    # Cluster Summary worksheet\n",
        "    cluster_summary = df_clusters.groupby('Cluster')['Keyword'].agg(['count', lambda x: ', '.join(x.head(5))]).reset_index()\n",
        "    cluster_summary.columns = ['Cluster', 'Number of Keywords', 'Top Keywords']\n",
        "    with pd.ExcelWriter('./LDA_output_files/output.xlsx', engine='openpyxl', mode='a') as writer:\n",
        "        cluster_summary.to_excel(writer, sheet_name='Cluster Summary', index=False)\n",
        "\n",
        "    # Topic Summary worksheet\n",
        "    topic_keywords = pd.DataFrame({'Topic': range(1, lda.n_components + 1)})\n",
        "    topic_keywords['Top Keywords'] = topic_keywords['Topic'].apply(lambda x: ', '.join([vectorizer.get_feature_names_out()[i] for i in lda.components_[x - 1].argsort()[:-n_top_words - 1:-1]]))\n",
        "    df_clusters_filtered = df_clusters[df_clusters['Topic'].isin(topic_keywords['Topic'])]\n",
        "    topic_keywords['Number of Keywords'] = df_clusters_filtered['Topic'].value_counts().loc[topic_keywords['Topic']].values\n",
        "    with pd.ExcelWriter('./LDA_output_files/output.xlsx', engine='openpyxl', mode='a') as writer:\n",
        "        topic_keywords.to_excel(writer, sheet_name='Topic Summary', index=False)\n",
        "\n",
        "    # Cluster-Topic Overlap worksheet\n",
        "    cluster_topic_overlap = pd.crosstab(df_clusters['Cluster'], df_clusters['Topic'])\n",
        "    with pd.ExcelWriter('./LDA_output_files/output.xlsx', engine='openpyxl', mode='a') as writer:\n",
        "        cluster_topic_overlap.to_excel(writer, sheet_name='Cluster-Topic Overlap')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxcHh0jTk9rq"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    whole_list = read_whole_list('scraped_seed_keywords_list.csv')\n",
        "    print(f\"Read {len(whole_list)} keywords from the whole list\")\n",
        "\n",
        "    min_cluster_size = 6\n",
        "    min_samples = 5\n",
        "    cluster_selection_epsilon = 0.5\n",
        "\n",
        "    print(\"Clustering keywords...\")\n",
        "    df_clusters, tfidf_matrix, vectorizer = cluster_keywords(whole_list, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
        "\n",
        "    n_components = 30\n",
        "    max_iter = 100\n",
        "    learning_method = 'online'\n",
        "\n",
        "    print(\"Performing topic modeling...\")\n",
        "    lda = topic_modeling(tfidf_matrix, n_components, max_iter, learning_method)\n",
        "\n",
        "    print(\"Visualizing clusters and topics...\")\n",
        "    visualize_clusters(df_clusters, tfidf_matrix)\n",
        "    visualize_topics(lda, vectorizer)\n",
        "\n",
        "    print(\"Creating output files...\")\n",
        "    create_output_files(df_clusters, lda, vectorizer)\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}